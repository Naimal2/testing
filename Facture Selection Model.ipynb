{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d3ad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8099  1.8784  1.9487  ... 1.0771  1.063   1.0477 ]\n",
      " [1.8042  1.8663  1.9334  ... 0.93248 0.91894 0.90425]\n",
      " [1.5693  1.6249  1.682   ... 0.81687 0.80471 0.79143]\n",
      " ...\n",
      " [2.8768  2.898   2.921   ... 0.77666 0.77487 0.77302]\n",
      " [2.7546  2.7826  2.8058  ... 0.60506 0.60352 0.6019 ]\n",
      " [3.1097  3.1569  3.2092  ... 0.78538 0.7833  0.78087]] [1 1 1 ... 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# Importing the dataset\n",
    "alldata=pd.read_csv('Strawberry_data_1_6.csv')\n",
    "X = alldata.iloc[:, :].values\n",
    "y = alldata.iloc[:, 224].values\n",
    "X = alldata.iloc[:, 1:223].values\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa7e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c88e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268fbe99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8099 , 1.8784 , 1.9487 , ..., 1.6885 , 1.6112 , 1.5394 ],\n",
       "       [1.8042 , 1.8663 , 1.9334 , ..., 1.7936 , 1.7    , 1.6106 ],\n",
       "       [1.5693 , 1.6249 , 1.682  , ..., 1.0665 , 1.0175 , 0.97257],\n",
       "       ...,\n",
       "       [2.8768 , 2.898  , 2.921  , ..., 2.5483 , 2.5202 , 2.4907 ],\n",
       "       [2.7546 , 2.7826 , 2.8058 , ..., 2.3703 , 2.3512 , 2.3306 ],\n",
       "       [3.1097 , 3.1569 , 3.2092 , ..., 3.122  , 3.0611 , 2.9983 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76507238",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a0877ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Create and train the SVM classifier\n",
    "svm = SVC(kernel = 'poly', random_state = 0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872ed88",
   "metadata": {},
   "source": [
    "# Derivative-based feature selection + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027505a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Compute Pearson correlation coefficients for each feature\n",
    "correlations = []\n",
    "for i in range(X.shape[1]):\n",
    "    corr, _ = pearsonr(X[:, i], y)\n",
    "    correlations.append(abs(corr))\n",
    "\n",
    "# Sort features based on correlation coefficient\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "sorted_X = X[:, sorted_indices]\n",
    "\n",
    "# Select top k features\n",
    "k = 10\n",
    "selected_features = sorted_X[:, :k]\n",
    "\n",
    "# Split data into train and test sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(X.shape[0] * split_ratio)\n",
    "X_train, X_test = selected_features[:split_index], selected_features[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Train SVM classifier\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84aa0c",
   "metadata": {},
   "source": [
    "# Derivative-based feature selection + ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c46496e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 360ms/step - loss: 0.1118 - accuracy: 1.0000\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Compute Pearson correlation coefficients for each feature\n",
    "correlations = []\n",
    "for i in range(X.shape[1]):\n",
    "    corr, _ = pearsonr(X[:, i], y)\n",
    "    correlations.append(abs(corr))\n",
    "\n",
    "# Sort features based on correlation coefficient\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "sorted_X = X[:, sorted_indices]\n",
    "\n",
    "# Select top k features\n",
    "k = 5\n",
    "selected_features = sorted_X[:, :k]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the ANN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_dim=k))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20ca3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aqeel\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Compute Pearson correlation coefficients for each feature\n",
    "correlations = []\n",
    "for i in range(X.shape[1]):\n",
    "    corr, _ = pearsonr(X[:, i], y)\n",
    "    correlations.append(abs(corr))\n",
    "\n",
    "# Sort features based on correlation coefficient\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "sorted_X = X[:, sorted_indices]\n",
    "\n",
    "# Select top k features\n",
    "k = 5\n",
    "selected_features = sorted_X[:, :k]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy=sklearn.metrics.accuracy_score(y_test,y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff7b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Compute Pearson correlation coefficients for each feature\n",
    "correlations = []\n",
    "for i in range(X.shape[1]):\n",
    "    corr, _ = pearsonr(X[:, i], y)\n",
    "    correlations.append(abs(corr))\n",
    "\n",
    "# Sort features based on correlation coefficient\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "sorted_X = X[:, sorted_indices]\n",
    "\n",
    "# Select top k features\n",
    "k = 5\n",
    "selected_features = sorted_X[:, :k]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "511fe61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Compute Pearson correlation coefficients for each feature\n",
    "correlations = []\n",
    "for i in range(X.shape[1]):\n",
    "    corr, _ = pearsonr(X[:, i], y)\n",
    "    correlations.append(abs(corr))\n",
    "\n",
    "# Sort features based on correlation coefficient\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "sorted_X = X[:, sorted_indices]\n",
    "\n",
    "# Select top k features\n",
    "k = 5\n",
    "selected_features = sorted_X[:, :k]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e21d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.2375\n",
      "Test Accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the PLS-DA model\n",
    "plsda = PLSRegression(n_components=2)\n",
    "plsda.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train_transformed = plsda.transform(X_train)\n",
    "X_test_transformed = plsda.transform(X_test)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred_train = np.argmax(X_train_transformed, axis=1)\n",
    "y_pred_test = np.argmax(X_test_transformed, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a5d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train SVM classifier on the reduced data\n",
    "svm = SVC()\n",
    "svm.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3673c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# Train SVM classifier on the reduced data\n",
    "svm = SVC()\n",
    "svm.fit(X_train_lda, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm.predict(X_test_lda)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d1d8bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy=sklearn.metrics.accuracy_score(y_test,y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eecb69d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.938\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "modelCV = LinearDiscriminantAnalysis()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a618d62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation average accuracy: 0.938\n",
      "[[ 9  0]\n",
      " [ 0 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0ab4d",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3dfa072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 0.6784 - accuracy: 0.5625\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6298 - accuracy: 0.6500\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5862 - accuracy: 0.7750\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5479 - accuracy: 0.8750\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5108 - accuracy: 0.9625\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4744 - accuracy: 0.9625\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4402 - accuracy: 0.9625\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4061 - accuracy: 0.9625\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3736 - accuracy: 0.9875\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3417 - accuracy: 0.9875\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an ANN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.round(y_pred).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c4230e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7624\\739292236.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# see how the training went\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# see how the training went\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Cross-Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ece22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Cross-Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
